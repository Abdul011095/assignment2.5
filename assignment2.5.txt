1)
 

cluster:
In a computer system, a cluster is a group of servers and other resources that act like a single system and enable high availability  in some cases load balancing and parallel processing


Hadoop cluster:A Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of unstructured data in a distributed computing environment
	       Such clusters run Hadoop's open source distributed processing software on low-cost commodity computers
               Typically one machine in the cluster is designated as the NameNode and another machine the as JobTracker these are the masters 
               The rest of the machines in the cluster act as both DataNode and TaskTracker these are the slaves 
               Hadoop clusters are often referred to as shared nothing systems because the only thing that is shared between nodes is the network that connects them
               Hadoop clusters are known for boosting the speed of data analysis applications
               They also are highly scalable 
               If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput 
               Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes which ensures that the data is not lost if one node fails
	       Large Hadoop Clusters are arranged in several racks
	       Network traffic between different nodes in the same rack is much more desirable than network traffic across the racks

=======================================================================================================================================================================================================
2)

component of Hadoop 1.x


NameNode :It contains the hadoop filesystem tree and metadata information about files and directories
          It is a software that can be run on commodity hardware
          The system having the namenode acts as the master server 

Secondary Namenode :performs house-keeping activities for namenode
                    periodic merging of namespace and edits
                    This is not a backup for Namenode

DataNode:stores actual data blocks of file in HDFS on its local disk
         sends signals to NAmeNode periodically called heartbeat to verify if its active
          they are the workhorse of the system

jobTracker:Job Tracker is used to assign MapReduce Tasks to Task Trackers in the Cluster of Nodes
           Job Tracker maintains all the Task Trackers status like Up/running, Failed, Recovered etc.

TaskTracker:Task Tracker executes the Tasks which are assigned by Job Tracker sends the status of those tasks to Job Tracker
            Runs individual map-reduce jobs on datanodes